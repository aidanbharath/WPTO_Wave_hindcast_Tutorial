{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "- **Section 1**: Dataset Description\n",
    "- **Section 2**: Installation and Setup of the REsource eXtraction Toolkit (rex)\n",
    "- **Section 3**: Working with the rex CLI to Retrieve Datasets in CSV Format\n",
    "- **Section 4**: Working with rex and Python to access the Datasets\n",
    "- **Section 5**: Data Processing and Plotting examples with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 1: Dataset Description\n",
    "\n",
    "This Wave Hindcast Dataset is dynamic, in that it will be improved with the addition of more US-EEZ regional data, extensions of the timeseries available and the inclusion of more *virtual buoys* with Direction Spectrum Data.  \n",
    "\n",
    "**Currently there are two major sub-datasets within the overarching dataset:** \n",
    "\n",
    "\n",
    "**1.** The high-spatial-resolution dataset (hereafter the *'spatial dataset'*) spans the U.S. Exclusive Economic Zone (EEZ) with an unstructured grid that has ~200 m resolution in shallow water. The time step resolution for this spatial data is 3-hours and spans 32 years, 01/01/1979  - 12/31/2010.\n",
    " \n",
    "\n",
    "**2.** The *'virtual buoy dataset'* is also available at specific locations within the large spatial domain. These virtual buoys span the same 32-years of the spatial dataset however the time resolution is 1-hour, and these data also include the wave directional spectrum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Spatial Dataset:\n",
    "\n",
    "- The spatial dataset is organized into yearly files (each file contains all variables and locations in that year) and they are located on AWS at: \n",
    "\n",
    "    **`/nrel/US_wave/US_wave_{year}.h5`**\n",
    "\n",
    "\n",
    "- Dataset variables included are indexed by **'coordinates'** (latitude and longitude), and a **'time_index'**\n",
    "    - Currently available years span (1979-2010) \n",
    "    - Latitude and Longitude Coordinates include those within the US west coast EEZ\n",
    "    \n",
    "\n",
    "- Dataset expansion will make availalbe hindcast data from all US coastal waters with an extended Hindcast time history range to 2020.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Included Spatial Variables: \n",
    "\n",
    "| Variable Name | Units |\n",
    "| :------------ | :---: |\n",
    "| energy_period | seconds |\n",
    "| maximum_energy_period | degrees_true |\n",
    "| mean_absolute_period | seconds |\n",
    "| mean_zero-crossing_period | seconds |\n",
    "| omni-directional_wave_power | Watts |\n",
    "| peak_period | seconds |\n",
    "| significant_wave_height | meters |\n",
    "| water_depth | meters |\n",
    "| spectral_width | none |\n",
    "| directionality_coefficient | none |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Virtual Buoy Dataset: (NOTE: add comment on changing dataover time)\n",
    "\n",
    "- The virtual buoy dataset is located on AWS at: \n",
    "\n",
    "    **`/nrel/US_wave/virtual_buoy/US_virtual_buoy_{year}.h5`**\n",
    "    \n",
    "\n",
    "- This dataset includes **all variables** in the spatial dataset, and also includes the directional wave spectrum (indexed by 'time_index','frequency', 'direction', and 'coordinates'):\n",
    "  \n",
    "| Variable Name | Units |\n",
    "| :------------ | :---: |\n",
    "| direction_wave_spectrum | meters<sup>2</sup>degree<sup>-1</sup>Hz<sup>-1</sup> |\n",
    "| maximum_energy_direction | degrees_true |\n",
    "| mean_wave_direction | degrees_true |\n",
    "| frequency_bin_edges | Hz |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 2: Installation and Setup or the REsource eXtraction Toolkit (rex)\n",
    "\n",
    "**In this section we will cover the process for install the rex package.** \n",
    "\n",
    "- rex offers a series of Command Line Interface (CLI) commands to easily access and download portions of the WPTO Wave Hindcast Dataset as CSV files and with it's implementation in **Python >=3.7**, analysis scripts can be designed to access data directly from the cloud eliminating the need to store data locally.\n",
    "\n",
    "**Topics covered in the section include:**\n",
    "- Installing rex via PIP of Conda into virtual environments\n",
    "- Configuring the HSDS sevice with the public API key to access the Datasets\n",
    "\n",
    "**Python installations for Windows, MacOS and Linux can be found here:**\n",
    "1. [Base Python]((https://www.python.org/downloads/)\n",
    "2. [Anaconda Distribution](https://docs.anaconda.com/anaconda/install/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Basic Installations of rex Using PIP or Conda\n",
    "\n",
    "**While using either pip or conda to install ``NREL-rex`` is personal preference it is recommended to create a 'working environment' when installing the code. The following examples show how to do this with pip and conda:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### With PIP:\n",
    "\n",
    "1. Create a new environment:\n",
    "  \n",
    "      ``python3 -m pip install --upgrade pip``\n",
    "      \n",
    "      ``pip3 install virtualenv``\n",
    "      \n",
    "      ``mkvirtualenv -p '/path/to/PYTHON_EXE' env_name``\n",
    "      \n",
    "2. Activate the environment:\n",
    "\n",
    "      ``workon env_name``\n",
    "      \n",
    "3. Install rex:\n",
    "\n",
    "      ``pip install NREL-rex``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### With Conda:\n",
    "\n",
    "  1. Create a new environment:\n",
    "  \n",
    "    ``conda create --name env_name``\n",
    "\n",
    "  2. Activate directory:\n",
    "  \n",
    "  ``conda activate env_name``\n",
    "\n",
    "  3. Install rex:\n",
    "  \n",
    "      1) ``pip install NREL-rex`` or\n",
    "      \n",
    "      2) ``conda install nrel-rex --channel=nrel``\n",
    "\n",
    "- NOTE: If you install using conda and want to use `HSDS <https://github.com/NREL/hsds-examples>`_you will also need to install h5pyd manually: ``pip install h5pyd``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cloning and Installing Python Packages for this Tutorial  \n",
    "\n",
    "**If the user intends to make use of the python based examples included in this tutorial, it will be necessary to install some extended python packages not included within rex.**\n",
    "\n",
    "To do this, it is simplest to clone or fork the github based **WPTO US Wave Hindcast Tutorial** directly from:\n",
    "- **[WPTO US Wave Hindcast Tutorial](https://github.com/aidanbharath/WPTO_Wave_hindcast_Tutorial.git)**\n",
    "\n",
    "An introduction to using ``git`` can be found [here](https://docs.github.com/en/github/getting-started-with-github) and with the ``virtualenv`` created in the previous example, run:\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Once complete the python installation active in the virtual environment can execute this tutorial in it entirety."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Installing and Configuring the HSDS service\n",
    "\n",
    "Next you'll need to configure h5pyd to access the data on HSDS:\n",
    "```\n",
    "$ hsconfigure\n",
    "```\n",
    "and enter at the prompt:\n",
    "\n",
    "```\n",
    "hs_endpoint = https://developer.nrel.gov/api/hsds   \n",
    "hs_username = None   \n",
    "hs_password = None   \n",
    "hs_api_key = 3K3JQbjZmWctY0xmIfSYvYgtIcM3CN0cb1Y2w9bf    \n",
    "```\n",
    "\n",
    "The example API key here is for demonstation and is rate-limited per IP. To get your own API key, visit https://developer.nrel.gov/signup/\n",
    "\n",
    "The example API key here is for demonstation and is rate-limited per IP. To get your own API key, visit https://developer.nrel.gov/signup/. Alternatively, you can add the above contents to the hsds configuration file: `~/.hscfg`\n",
    "\n",
    "Use [Jupyter Notebook or Jupyter Lab](https://jupyter.org) to view and modify this example notebook. For example, after following the steps above, start a jupyter notebook by:\n",
    "```\n",
    "$ jupyter notebook\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 3: Working with the rex CLI to Access the Datasets\n",
    "\n",
    "The rex CLI offers simple and covenient commands to access and download variable data from the WPTO Wave Hindcast Dataset. The datasets are saved locally in CSV formats which can easily be loaded into Excel, MatLab or any other post-processing tool. \n",
    "\n",
    "The following section of this tutorial will provide examples of how to work with the rex CLI:\n",
    " - **Example 3.1**: The WaveX Command Line Interface\n",
    " - **Example 3.2**: Accessing and Downloading a CSV for a Single Site\n",
    " - **Example 3.3**: Accessing and Downloading a CSV for multiple locations\n",
    " - **Example 3.4**: Accessing and Downloading a CSV for data from multiple years\n",
    " - **Example 3.5**: Accessing and Downloading a CSV for the Directional Wave Spectrum Data from the Virtual Buoys\n",
    "\n",
    "The goal is to provide a means of accessing data that can then be process by any other means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example 3.1: The WaveX Command Line Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "WaveX dataset --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example 3.2: Accessing and Downloading a CSV for a Single Site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "WaveX -h5 '/nrel/US_wave/US_wave_1990.h5' -o './' dataset --help -d 'significant_wave_height' -ll '44.624076' '-124.280097'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls significant_wave_height-413889.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "head significant_wave_height-413889.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from pandas import DataFrame\n",
    "a = DataFrame(array([(44.624076,-124.280097),(45.624076,-125.280097)]),columns=['latitude','longitude'])\n",
    "\n",
    "a.to_csv('./sites.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example 3.3: Accessing and Downloading a CSV for Multiple Locations\n",
    "\n",
    "Calling multiple locations will have the data compiled into a single file.\n",
    "\n",
    "This CLI command works from a '.csv' or 'gid' with latitude, longitude pairs to pull data from AWS and and file name needs to be passed as an option to 'multi-site'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "WaveX -h5 '/nrel/US_wave/US_wave_1990.h5' -o './'  multi-site -s './sites.csv' -d 'significant_wave_height' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example 3.4: Accessing and Downloading a CSV for Data from Multiple Years\n",
    "\n",
    "Concatonating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "MultiYearX -v -h5 '/nrel/US_wave/US_wave_199*.h5' -o './' -res 'Wave' -yrs [1995,1996] -hsds dataset -d 'significant_wave_height'  site  -ll '44.624076' '-124.280097'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example 3.5: Accessing and Downloading a CSV of the Directional Wave Spectrum Data from the Virtual Buoys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 4: Working with rex and Python to access the Datasets\n",
    "\n",
    "In the following section we provide examples to access WPTO Wave Hindcast Dataset using the rex toolkit within a python environment.\n",
    "\n",
    "Examples provided include:\n",
    "- **Example 4.1**: Accessing dataset metadata parameters:\n",
    "    - *meta-data*\n",
    "    - *time_index*\n",
    "    - *coordinates* (latitude/longitude pairs)\n",
    "- **Example 4.2**: Extracting Data from a single site\n",
    "- **Example 4.3**: Extracting Data from multiple sites\n",
    "- **Example 4.4**: Extracting Data over multiple years\n",
    "    - Using Filename Wildcards\n",
    "    - Specifying Years\n",
    "- **Example 4.5**: Working with the Virtual Buoy Dataset\n",
    "    - Accessing *meta-data* parameters\n",
    "    - Accessing Bulk Parameters over Multiple Years\n",
    "- **Example 4.6**: Viewing the Directional Wave Spectrum data\n",
    "- **Example 4.7**: Exporting Python Datasets to various other types\n",
    "\n",
    "The goal is to provide an overview of how python can be used to interact with the available datasets. Each individual example below is self contained and can be copied to another script if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 4.1: Accessing Dataset metadata Parameters\n",
    "\n",
    "The following examples illustrate basic examples using NREL-rex to access and download parts of the WPTO Wave Hindcast dataset.\n",
    "\n",
    "Datasets are returned as Pandas DataFrame objects. See the Pandas [documentation](https://pandas.pydata.org/pandas-docs/stable/) \n",
    "for more information about working with Pandas objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### \"*meta-data*\" Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from rex import WaveX\n",
    "waveFile = f'/nrel/US_wave/US_wave_1990.h5'\n",
    "\n",
    "with WaveX(waveFile, hsds=True) as waves:\n",
    "    # meta is an object that contains location information for each data point\n",
    "    meta = waves.meta \n",
    "meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### \"*time_index*\" Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rex import WaveX\n",
    "waveFile = f'/nrel/US_wave/US_wave_1990.h5'\n",
    "\n",
    "with WaveX(waveFile, hsds=True) as waves:\n",
    "    # time_index contains all of the years within the file\n",
    "    time_index = waves.time_index \n",
    "time_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### \"*coordinates*\" (latitude/longitude pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rex import WaveX\n",
    "waveFile = f'/nrel/US_wave/US_wave_1990.h5'\n",
    "\n",
    "with WaveX(waveFile, hsds=True) as waves:\n",
    "    # coordinates contains all the lat/lon pars within the dataset\n",
    "    coordinates = waves.coordinates \n",
    "coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example 4.2: Extracting data from a Single Site\n",
    "A single latitude/longitude pair can be given to extract data nearest to that location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the timeseries of significant_wave_height closest to a coordinate pair\n",
    "from rex import WaveX\n",
    "waveFile = f'/nrel/US_wave/US_wave_1990.h5'\n",
    "lat_lon = (44.624076,-124.280097)\n",
    "parameters = 'significant_wave_height'\n",
    "    \n",
    "with WaveX(waveFile, hsds=True) as waves:\n",
    "    swh_single = waves.get_lat_lon_df(parameters, lat_lon)\n",
    "swh_single"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example 4.3: Extracting data from Multiple Sites\n",
    "A list of latitude/longitude pairs can be passed to extract data from multiple sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the timeseries of significant_wave_height closest to multiple coordinate pairs\n",
    "from rex import WaveX\n",
    "waveFile = '/nrel/US_wave/US_wave_1990.h5'\n",
    "lat_lon = [(44.624076,-124.280097),(43.489171,-125.152137)] # set lat_lon to a list of lat/lon pairs\n",
    "parameters = 'significant_wave_height'\n",
    "    \n",
    "with WaveX(waveFile, hsds=True) as waves:\n",
    "    swh_multi = waves.get_lat_lon_df(parameters, lat_lon)\n",
    "swh_multi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example 4.4: Extracting Data over Multiple Years\n",
    "The previous examples return data for a single year only. To extract data over a number of years, use the `MultiYearWaveX` class which can accept wildcards witihin the filename:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Using filename Wildcards:\n",
    "\n",
    "Using a wildcard in the dataset filename will tell `'MultiYearWaveX'` to access all datasets matching the written specified filename. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveFile_wildcard = f'/nrel/US_wave/US_wave_199*.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from rex import MultiYearWaveX # Yearly concatonation requires the MultiYearResource function\n",
    "lat_lon = (44.624076,-124.280097)\n",
    "parameters = 'significant_wave_height'\n",
    "    \n",
    "with MultiYearWaveX(waveFile_wildcard,hsds=True) as mYears:\n",
    "    swh_multi_year = mYears.get_lat_lon_df(parameters, lat_lon)\n",
    "swh_multi_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Specifying Years to Access:\n",
    "\n",
    "We can pass a list of specific years to `MultiYearWaveX` to access. \n",
    "\n",
    "Note in this case we still need to specify the dataset's file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveFiles = f'/nrel/US_wave/US_wave_*.h5'\n",
    "years = [1989,1990,1991,1992,1993,2005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from rex import MultiYearWaveX # Yearly concatonation requires the MultiYearResource function\n",
    "lat_lon = (44.624076,-124.280097)\n",
    "parameters = 'significant_wave_height'\n",
    "    \n",
    "with MultiYearWaveX(waveFiles,years=years,hsds=True) as mYears:\n",
    "    swh_multi_year = mYears.get_lat_lon_df(parameters, lat_lon)\n",
    "swh_multi_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example 4.5: Working with the Virtual Buoy Datasets\n",
    "\n",
    "Virtual buoys were created during the larger UnSWAN model runs and contain 1-hour timestep data with directional spectrum data\n",
    "\n",
    "These data can be accessed using the same basic approaches described above, but using a different path that points to these data: `/nrel/US_wave/virtual_buoy/US_virtual_buoy_{year}.h5`\n",
    "\n",
    "The Virtual Buoy data is structured in the same way as the Spatial Dataset. Accessing the data is accomplished in the same way, and differs only in the dataset's directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Virtual Buoy \"*time_index*\" Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rex import WaveX\n",
    "vBuoy = f'/nrel/US_wave/virtual_buoy/US_virtual_buoy_1995.h5'\n",
    "\n",
    "with WaveX(vBuoy, hsds=True) as waves:\n",
    "    time_index = waves.time_index \n",
    "time_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Virtual Buoy Bulk Parameter Access over Multiple Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rex import MultiYearWaveX\n",
    "multi_year_vBuoy = f'/nrel/US_wave/virtual_buoy/*.h5'\n",
    "years = [1979,1987,1999,2008]\n",
    "lat_lon = (44.624076,-124.280097)\n",
    "parameters = 'significant_wave_height'\n",
    "    \n",
    "with MultiYearWaveX(multi_year_vBuoy,years=years,hsds=True) as mYears:\n",
    "    swh_buoy = mYears.get_lat_lon_df(parameters, lat_lon)\n",
    "swh_buoy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example 4.6: Accessing and Viewing with the Virtual Buoy Directional Wave Spectum\n",
    "\n",
    "Directional wave spectrum data is more complicated to work with in that we have the extra dimentions of frequency and direction within the dataset. These dataset are returned from HSDS as [Pandas Multi-index Dataframes](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html).\n",
    "\n",
    "This format flattens the 3-dimensional array along a single axis so the data itself can still be easily displayed and interpreted.\n",
    "\n",
    "This examples shows the form of the `'directional_wave_spectrum'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rex import WaveX\n",
    "vBuoy = f'/nrel/US_wave/virtual_buoy/US_virtual_buoy_1995.h5'\n",
    "lat_lon = (44.624076,-124.280097)\n",
    "spc_name = 'directional_wave_spectrum'\n",
    "\n",
    "with WaveX(vBuoy, hsds=True) as waves:\n",
    "    spc_buoy = waves.get_lat_lon_df(spc_name, lat_lon)\n",
    "spc_buoy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example 4.7: Exporting the Datasets from Python to Various other Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 5: Data-Processing and Plotting Examples with Python\n",
    "\n",
    "This section of the tutorial offers basic introductions to working with the data within python. \n",
    "\n",
    "Examples include:\n",
    "\n",
    "- **Example 5.1**: Plotting a Year of Data from the Spatial Dataset\n",
    "- **Example 5.2**: Creating a *Typical* year Dataset\n",
    "- **Example 5.3**: Working with the Directional Wave Spectrum and Polar Plots \n",
    "- **Example 5.4**: Creating and Plotting a Joint Probability Distribution\n",
    "- **Example 5.5**: Creating and Plotting a Directionaly Dependent JPD\n",
    "\n",
    "The goal of these examples are to offer a starting point for more elaborate and extensive use of the datasets in further studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example 5.1: Plotting a Year of Data from the Spatial Dataset\n",
    "\n",
    "Extracting a year of data now should be quick and easy matter of using **rex** to gather, download and organise the data from AWS servers. \n",
    "\n",
    "What follows below is a *'TwinX'* plotting method to view a comparison between Significant Wave Height and Omni-Directional Wave Power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rex import WaveX\n",
    "waveFile = f'/nrel/US_wave/US_wave_1995.h5'\n",
    "lat_lon = (44.624076,-124.280097)\n",
    "swh_name, owp_name = 'significant_wave_height', 'omni-directional_wave_power'\n",
    "    \n",
    "with WaveX(waveFile, hsds=True) as waves:\n",
    "    swh = waves.get_lat_lon_df(swh_name, lat_lon)\n",
    "    owp = waves.get_lat_lon_df(owp_name, lat_lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot makes use of python's **matplotlib** package to format and display the data just downloaded in the previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format ='retina' \n",
    "%matplotlib widget \n",
    "\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "font = {'family' : 'DejaVu',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 12}\n",
    "\n",
    "mpl.rc('font', **font)\n",
    "\n",
    "host = host_subplot(111)\n",
    "par = host.twinx()\n",
    "\n",
    "host.set_xlabel(\"Time\")\n",
    "host.set_ylabel('Significant Wave Height (m)')\n",
    "par.set_ylabel('Omni-Directional Wave Power (kW/m)')\n",
    "\n",
    "p1, = host.plot(swh.index, swh.values, label='Significant Wave Height')\n",
    "p2, = par.plot(owp.index, owp.values/1000, label='Omni-Directional Wave Power')\n",
    "\n",
    "leg = plt.legend()\n",
    "\n",
    "host.yaxis.get_label().set_color(p1.get_color())\n",
    "leg.texts[0].set_color(p1.get_color())\n",
    "\n",
    "par.yaxis.get_label().set_color(p2.get_color())\n",
    "leg.texts[1].set_color(p2.get_color())\n",
    "\n",
    "plt.title('WPTO Wave Hindacast Data for 1995')\n",
    "plt.grid(linestyle=':')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This plotting routine makes use of the data available within the Spatial dataset and so minimal changes are needed in order to plot different variable combinations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example 5.2: Creating an Average Year Dataset\n",
    "\n",
    "We can apply specific statistical functions across several years of data from the data set to create representitive datasets. \n",
    "\n",
    "What follows below is an example of calculating an average year dataset based on 5 years of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rex import MultiYearWaveX\n",
    "from numpy import arange\n",
    "\n",
    "waveFile = f'/nrel/US_wave/US_wave_19*.h5'\n",
    "years = arange(1995,2000,1)\n",
    "lat_lon = (44.624076,-124.280097)\n",
    "owp_name = 'omni-directional_wave_power'\n",
    "    \n",
    "with MultiYearWaveX(waveFile,years=years, hsds=True) as waves:\n",
    "    owp = waves.get_lat_lon_df(owp_name, lat_lon)\n",
    "owp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the [Pandas Group-by Functionality](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html) we can group the entire dataset in a wide variety of ways and apply builtin or user-defined functions.\n",
    "\n",
    "In the example below, we group the 5 year dataset by month, day and hour and take the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = owp.index\n",
    "typical_owp_year = owp.groupby([idx.month,idx.day,idx.hour]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can then plot the resulting representitive year of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format ='retina' \n",
    "%matplotlib widget \n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "from pandas import to_datetime\n",
    "\n",
    "years = mdates.YearLocator()  # every year\n",
    "days = mdates.DayLocator()\n",
    "months = mdates.MonthLocator()  # every month\n",
    "years_fmt = mdates.DateFormatter('%m')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(owp.loc['1996'].index, typical_owp_year/1000, \n",
    "        label='Averaged Year from 1990-1999')\n",
    "\n",
    "ax.plot(owp.loc['1996'].index, owp.loc['1996']/1000, \n",
    "        label='Modelled Year 1996')\n",
    "\n",
    "ax.xaxis.set_major_locator(months)\n",
    "ax.xaxis.set_major_formatter(years_fmt)\n",
    "ax.xaxis.set_minor_locator(days)\n",
    "\n",
    "ax.format_xdata = mdates.DateFormatter('%m')\n",
    "plt.ylabel('Omni-Directional Wave Power (kW/m)')\n",
    "plt.xlabel('Time')\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "plt.title('Average Year Omni-Directional Wave Power')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example 5.3: Working with the Directional Wave Spectrum and Polar Plots \n",
    "\n",
    "Directional wave spectrum data is more complicated to work with in that we have the extra dimentions of frequency and direction within the dataset. These dataset are returned from HSDS as [Pandas Multi-index Dataframes](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html) and can be analyzed following that syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from rex import WaveX\n",
    "\n",
    "waveFile = f'/nrel/US_wave/virtual_buoy/US_virtual_buoy_1995.h5'\n",
    "lat_lon = (44.624076,-124.280097)\n",
    "spc_name = 'directional_wave_spectrum'\n",
    "\n",
    "with WaveX(waveFile, hsds=True) as waves:\n",
    "    spc = waves.get_lat_lon_df(spc_name, lat_lon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "This examples gives a method for creating a polar plot of a yearly mean of directional spectrum data. \n",
    "\n",
    "One processing step below, fills the data gap at 0<sup>o</sup> and 2pi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import unique, array, radians, hstack, pi, newaxis, mean, meshgrid\n",
    "\n",
    "# pull degree values from Datasets and convert to radians\n",
    "directions = hstack([unique(radians(spc.index.get_level_values(level=2)))])\n",
    "\n",
    "# add angle 0 and 2pi\n",
    "azimuths = hstack([array(0),directions,2*pi])\n",
    "zeniths = unique(spc.index.get_level_values(level=1))\n",
    "\n",
    "# create the 2D meshgrid for plots and correctly orient the angles\n",
    "r, theta = meshgrid(zeniths, azimuths)\n",
    "theta = 90-theta\n",
    "\n",
    "# Calculate the mean across time and replace 0 values (for log scale) \n",
    "yearly_mean = spc.mean(level=(1,2)).values.reshape(zeniths.shape[0],azimuths.shape[0]-2)\n",
    "yearly_mean[yearly_mean<=0] = 1e-9\n",
    "\n",
    "# populate the 0 and 2pi directions of the data\n",
    "edges = mean([yearly_mean[:,0],yearly_mean[:,-1]],axis=0)\n",
    "yearly_mean = hstack([edges[:,newaxis],hstack([yearly_mean,edges[:,newaxis]])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "With the corrections made we can create the plot of directional wave spectrum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notebook commands are not needed in separate scripts\n",
    "%config InlineBackend.figure_format ='retina' \n",
    "%matplotlib widget \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import LogLocator\n",
    "import matplotlib as mpl\n",
    "\n",
    "font = {'family' : 'DejaVu Sans',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 10}\n",
    "\n",
    "mpl.rc('font', **font)\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw=dict(projection='polar'))\n",
    "cs = ax.contourf(theta, r, yearly_mean.T, \n",
    "                 locator=LogLocator(numticks=50)\n",
    "                )\n",
    "ax.set_theta_zero_location(\"N\")\n",
    "ax.set_xticklabels(['N', 'NW', 'W', 'SW', 'S', 'SE', 'E', 'NE'])\n",
    "\n",
    "label_position = ax.get_rlabel_position()\n",
    "ax.text(\n",
    "        radians(label_position+10),ax.get_rmax()/2.,'Frequency (Hz)',\n",
    "        rotation=label_position-90,ha='center',va='center'\n",
    "        )\n",
    "ax.set_title(\"Directional Wave Spectrum\", va='bottom')\n",
    "\n",
    "cbar = plt.colorbar(cs)\n",
    "cbar.ax.set_ylabel(r'Frequency Amplitude (m^2/Hz/deg)', rotation=270,\n",
    "                   fontsize=font['size'],fontweight=font['weight']\n",
    "                  )\n",
    "plt.tight_layout()\n",
    "plt.grid(linestyle=':',color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example 5.4: Creating and Plotting a Joint Probability Distribution\n",
    "\n",
    "We can quickly calculate a JPD based on the 1-hr bulk parameters from the Virtual Buoy datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rex import WaveX\n",
    "\n",
    "waveFile = f'/nrel/US_wave/virtual_buoy/US_virtual_buoy_1995.h5'\n",
    "lat_lon = (44.624076,-124.280097)\n",
    "Hm0_name = 'significant_wave_height'\n",
    "Te_name = 'peak_period'\n",
    "Dir_name = 'mean_wave_direction'\n",
    "\n",
    "with WaveX(waveFile, hsds=True) as waves:\n",
    "    Hm0 = waves.get_lat_lon_df(Hm0_name, lat_lon)\n",
    "    Te = waves.get_lat_lon_df(Te_name, lat_lon)\n",
    "    Dir = waves.get_lat_lon_df(Dir_name, lat_lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "For the JPD we can make use of numpy's builtin functions to handle much of the heavy lifting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import histogramdd, array, arange\n",
    "from pandas import DataFrame\n",
    "\n",
    "# Generate bins for Hm0 and Te, input format (start, stop, step_size)\n",
    "Hm0_bins = arange(0, Hm0.values.max() + 0.5, 0.5)    \n",
    "Te_bins = arange(0, Te.values.max() + 1, 1)\n",
    "\n",
    "# Combine data for better handling\n",
    "jpd_data = array([Hm0.values.flatten(),Te.values.flatten()]).T\n",
    "\n",
    "# Calculate the bin centers of the data\n",
    "Hm0_center = array([\n",
    "                    mean([Hm0_bins[i+1],Hm0_bins[i]]) \n",
    "                    for i in range(Hm0_bins.shape[0]-1)\n",
    "                ])\n",
    "Te_center = array([\n",
    "                    mean([Te_bins[i+1],Te_bins[i]]) \n",
    "                    for i in range(Te_bins.shape[0]-1)\n",
    "                ])\n",
    "\n",
    "# Calculate the JPD for Hm0 and Te and pack into a DataFrame\n",
    "probability, edges = histogramdd(jpd_data,bins=[Hm0_bins,Te_bins],density=True)\n",
    "jpd = DataFrame(probability, index=Hm0_center, columns=Te_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We can then plot the resulting JPD using matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notebook commands are not needed in separate scripts\n",
    "%config InlineBackend.figure_format ='retina' \n",
    "%matplotlib widget \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import arange\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "\n",
    "im = ax.imshow(jpd, origin='lower', aspect='auto')\n",
    "\n",
    "cbar = plt.colorbar(im)\n",
    "cbar.set_label('Probability Density (1/(sec*m)', rotation=270, labelpad=15)\n",
    "\n",
    "ax.set_xlabel('Te (seconds)')\n",
    "ax.set_ylabel('Hm0 (meters)')\n",
    "\n",
    "ax.set_xticks(arange(len(jpd.columns)))\n",
    "ax.set_yticks(arange(len(jpd.index)))\n",
    "ax.set_xticklabels(jpd.columns)\n",
    "ax.set_yticklabels(jpd.index)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Joint Probability Distribution (JPD)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example 5.5: Creating and Plotting a Directionaly Dependent JPD\n",
    "\n",
    "As an extension, we can catagorize our JPD calculation by the `'mean_wave_direction'` which can also be found in the *Virtual Buoy* Datasets. This is shown in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rex import WaveX\n",
    "\n",
    "waveFile = f'/nrel/US_wave/virtual_buoy/US_virtual_buoy_1995.h5'\n",
    "lat_lon = (44.624076,-124.280097)\n",
    "Hm0_name = 'significant_wave_height'\n",
    "Te_name = 'peak_period'\n",
    "Dir_name = 'mean_wave_direction'\n",
    "\n",
    "with WaveX(waveFile, hsds=True) as waves:\n",
    "    Hm0 = waves.get_lat_lon_df(Hm0_name, lat_lon)\n",
    "    Te = waves.get_lat_lon_df(Te_name, lat_lon)\n",
    "    Dir = waves.get_lat_lon_df(Dir_name, lat_lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Generalized JPD Calculation using Direction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import histogramdd, array, arange, mean\n",
    "\n",
    "# Generate bins for Hm0, Te and Direction\n",
    "Hm0_bins = arange(0, Hm0.values.max() + 0.5, 0.5)    \n",
    "Te_bins = arange(0, Te.values.max() + 1, 1)\n",
    "Dir_bins = arange(0, Dir.values.max() + 10, 10)\n",
    "\n",
    "# Combine data for better handling\n",
    "jpd_3d = array([\n",
    "                    Hm0.values.flatten(),\n",
    "                    Te.values.flatten(),\n",
    "                    Dir.values.flatten()\n",
    "                ]).T\n",
    "\n",
    "# Calculate the bin centers of the data\n",
    "Hm0_center = array([\n",
    "                    mean([Hm0_bins[i+1],Hm0_bins[i]]) \n",
    "                    for i in range(Hm0_bins.shape[0]-1)\n",
    "                ])\n",
    "Te_center = array([\n",
    "                    mean([Te_bins[i+1],Te_bins[i]]) \n",
    "                    for i in range(Te_bins.shape[0]-1)\n",
    "                ])\n",
    "Dir_center = array([\n",
    "                    mean([Dir_bins[i+1],Dir_bins[i]]) \n",
    "                    for i in range(Dir_bins.shape[0]-1)\n",
    "                ])\n",
    "\n",
    "\n",
    "# Calculate the JPD for Hm0, Te, and Dir \n",
    "probability, edges = histogramdd(jpd_3d,bins=[Hm0_bins,Te_bins,Dir_bins],density=True)\n",
    "\n",
    "# The JPD Now includes the Directional arguement\n",
    "print(f'Shape of the 3 component JPD: {probability.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "To check the consistency of the probability density calculation we can perform a Volumn integral check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import diff, einsum, sum\n",
    "d = diff\n",
    "x, y, z = edges\n",
    "\n",
    "integral = sum(probability*einsum('i,j,k->ijk',d(x),d(y),d(z)))\n",
    "\n",
    "print(f'Integral of the Hm0, Te, Direction JPD: {integral}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We can add interactivity to the JPD to allow use to view data over the direction bins we have added to the JPB. In this case we can get a sense of the yearly JPD of various sea-states coming from a specific direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format ='retina' \n",
    "# notebook commands are not needed in separate scripts\n",
    "%matplotlib widget \n",
    "# notebook commands are not needed in separate scripts\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import arange\n",
    "from matplotlib.widgets import Slider\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.subplots_adjust(right=0.8, bottom=0.25)\n",
    "\n",
    "d = 0\n",
    "plot_jpd = probability[:,:,d]\n",
    "\n",
    "im = ax.imshow(plot_jpd, origin='lower', aspect='auto')\n",
    "\n",
    "axcolor = 'lightgoldenrodyellow'\n",
    "axDir = plt.axes([0.3, 0.075, 0.45, 0.03], facecolor=axcolor)\n",
    "\n",
    "newD = Slider(axDir, 'Income Wave\\n Direction', 5, 355, valinit=d, valstep=10)\n",
    "\n",
    "def update(val):\n",
    "    d = int(newD.val/10)\n",
    "    im.set_data(probability[:,:,d])\n",
    "    fig.canvas.draw()\n",
    "\n",
    "newD.on_changed(update)\n",
    "\n",
    "cax = fig.add_axes([0.82, 0.3, 0.03, 0.5])\n",
    "cbar = fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "\n",
    "cbar.set_label('Probability Density (1/(sec*m*deg)', rotation=270, labelpad=15)\n",
    "\n",
    "ax.set_xlabel('Te (seconds)')\n",
    "ax.set_ylabel('Hm0 (meters)')\n",
    "\n",
    "ax.set_xticks(arange(len(Te_center)))\n",
    "ax.set_yticks(arange(len(Hm0_center)))\n",
    "ax.set_xticklabels(Te_center,rotation=45)\n",
    "ax.set_yticklabels(Hm0_center)\n",
    "\n",
    "fig.suptitle('Joint Probability Density\\n of Hm0 and Te per Direction')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example 5.6: Creating a Spatial for a Specific Time\n",
    "\n",
    "We can access and download large regions of the dataset using rex. \n",
    "\n",
    "The following example shows how to plot a single time step of the full spatial dataset along the West Coast of the US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rex import WaveX\n",
    "from pandas import to_datetime\n",
    "\n",
    "waveFile = f'/nrel/US_wave/US_wave_1986.h5'\n",
    "Hm0_name = 'significant_wave_height'\n",
    "timestep = to_datetime('1986-08-07 12:00:00',utc=True)\n",
    "\n",
    "with WaveX(waveFile, hsds=True) as waves:\n",
    "    Hm0 = waves.get_timestep_map(Hm0_name, timestep)\n",
    "Hm0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopandas import GeoDataFrame\n",
    "from shapely.geometry import Point\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(Hm0['longitude'], Hm0['latitude'])]\n",
    "Hm0 = Hm0.drop(['longitude', 'latitude'], axis=1)\n",
    "gHm0 = GeoDataFrame(Hm0, crs=\"EPSG:3857\", geometry=geometry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "gHm0.iloc[0::1].plot(column='significant_wave_height',        \n",
    "                vmin = 0,\n",
    "                vmax = 3,\n",
    "                alpha = 0.2,\n",
    "                s = 3,\n",
    "                ax=ax,\n",
    "               legend=True,\n",
    "               legend_kwds={'label': \"Significant Wave Height\",\n",
    "                                 'orientation': \"vertical\"})\n",
    "ax.set_title('Significant Wave Height')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_xlabel('Longitude')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "In this tutorial we have covered several important topics when looking to work with the **WPTO Wave Hindcast Datasets**. In summary we have covered:\n",
    "- Installation of the REsource eXtraction Toolkit (rex)\n",
    "- Using the rex CLI to pull raw data from the AWS HSDS data servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "rise": {
   "font size": "5",
   "scroll": true,
   "theme": "sky",
   "transition": "zoom"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
